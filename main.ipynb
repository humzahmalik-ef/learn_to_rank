{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb6b1e53",
   "metadata": {},
   "source": [
    "# Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf1715a4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_svmlight_file\n",
    "# from tensorflow_serving.apis import input_pb2 # KEEP THIS COMMENTED OUT FIRST!!\n",
    "import tensorflow as tf\n",
    "import tensorflow_ranking as tfr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4e7117",
   "metadata": {},
   "outputs": [],
   "source": [
    "import modin.pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f26679b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow_serving.apis import input_pb2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787d9f16",
   "metadata": {},
   "source": [
    "# Feature List Extraction and Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128b36b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_features(features_path):\n",
    "    \n",
    "    '''This function processes and creates our feature columns descriptions'''\n",
    "    \n",
    "    # Read in the features file\n",
    "    features = pd.read_csv(features_path)\n",
    "    \n",
    "    # Create new header and replace spaces with underscore\n",
    "    new_header = features.iloc[0].str.replace(' ','_')\n",
    "    \n",
    "    # Remove the first row which is now the new header\n",
    "    features = features[1:]\n",
    "    \n",
    "    # Set new headers\n",
    "    features.columns = new_header\n",
    "    \n",
    "    # Only the first cell for each category is filled. Using forward will\n",
    "    # will allow me to map each category to their sub-categories located\n",
    "    # in the stream column \n",
    "    features['feature_description'] = features['feature_description'].ffill()\n",
    "    \n",
    "    # Replacing characters to allign with TensorFlows regex requirements\n",
    "    character_removal = [' ', '(', ')', '*']\n",
    "    for char in character_removal:\n",
    "        features['feature_description'] = features['feature_description'].str.replace(char, '_')\n",
    "        features['stream'] = features['stream'].astype(str).str.replace(char, '_')\n",
    "        \n",
    "    # Setting column type to string for mapping within the load_rename_save function\n",
    "    features['feature_id'] = features['feature_id'].astype(str)\n",
    "    \n",
    "    # Creating new column to map features to existing dataset\n",
    "    features['cols'] = 'string'\n",
    "    \n",
    "    # Looping over all features and creating new column name\n",
    "    for idx in range(len(features)):\n",
    "        if str(features.iloc[idx]['stream']) != 'nan':\n",
    "            features['cols'].iloc[idx] = features['feature_description'].iloc[idx] + '_' + features['stream'].iloc[idx]\n",
    "        else:\n",
    "            features['cols'].iloc[idx] = features['feature_description'].iloc[idx]\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bec835c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get the current working directory\n",
    "current_working_directory = os.getcwd()\n",
    "\n",
    "# Construct the path to the features.csv file\n",
    "features_path = os.path.join(current_working_directory, \"data\", \"features.csv\")\n",
    "\n",
    "# Run feature preprocessing\n",
    "features_df=preprocess_features(features_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb0e604",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_relevance_qid(df):\n",
    "    # Rename the first two columns\n",
    "    df.rename(columns={0: 'relevance', 1: 'qid'}, inplace=True)\n",
    "    \n",
    "    # Rename the rest of the columns\n",
    "    new_column_names = {i: i-1 for i in df.columns if isinstance(i, int) and i not in [0, 1]}\n",
    "    df.rename(columns=new_column_names, inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def drop_column_137(df):\n",
    "    # Check if column 137 exists in the dataframe\n",
    "    if 137 in df.columns:\n",
    "        df.drop(columns=[137], inplace=True)\n",
    "    else:\n",
    "        print(\"Column 137 does not exist in the dataframe.\")\n",
    "    return df\n",
    "\n",
    "def rename_cols(df, features):\n",
    "        \n",
    "    for col in df.columns:\n",
    "        \n",
    "        if col != 'relevance' and col != 'qid':\n",
    "        \n",
    "            associated_col_value=features.loc[features['feature_id'] == str(col), 'cols'][0]\n",
    "\n",
    "            df.rename(columns={col:associated_col_value}, inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "def replace_colon_values(df):\n",
    "    # Iterate through each cell in the DataFrame\n",
    "    for column in df.columns:\n",
    "        df[column] = df[column].apply(lambda x: x.split(':')[1] if isinstance(x, str) and ':' in x else x)\n",
    "    return df\n",
    "\n",
    "\n",
    "def full_preprocess_pipeline(df, features):\n",
    "    \n",
    "    # Rename cols 0 and 1 to relevancy and qid\n",
    "    df=replace_relevance_qid(df)\n",
    "    \n",
    "    # Drop column 137 due to entirely Null values\n",
    "    df=drop_column_137(df)\n",
    "    \n",
    "    # Rename columns using feature dataframe\n",
    "    df=rename_cols(df, features)\n",
    "    \n",
    "    # Remove colons\n",
    "    df=replace_colon_values(df)\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba188f98",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Base directory path\n",
    "data_dir = os.path.join(current_working_directory, \"data\")\n",
    "\n",
    "# Folders within the base directory\n",
    "folders = [f'Fold{i}' for i in range(1, 6)]\n",
    "\n",
    "# Process each file in each folder\n",
    "for folder in folders:\n",
    "    folder_path = os.path.join(data_dir, folder)\n",
    "    for filename in os.listdir(folder_path):\n",
    "        \n",
    "        print(f\"On: {filename}\")\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        \n",
    "        if os.path.isfile(file_path) and file_path.endswith('.txt'):\n",
    "        \n",
    "            # Read the file\n",
    "            df = pd.read_csv(file_path, sep=\" \", header=None)\n",
    "            \n",
    "            # Preprocess the dataframe\n",
    "            df = full_preprocess_pipeline(df, features)\n",
    "            \n",
    "            print(df.head())\n",
    "            \n",
    "            # Save the preprocessed dataframe\n",
    "            preprocessed_file_path = file_path.replace('.txt', '_preprocessed.csv')\n",
    "            \n",
    "            df.to_csv(preprocessed_file_path, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a51c40",
   "metadata": {},
   "source": [
    "# Building TFRecords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27c0384",
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_to_path(df, file_name):\n",
    "\n",
    "    # Construct the path to the features.csv file\n",
    "    combined_path = os.path.join(current_working_directory, \"data\", \"Combined\")\n",
    "    \n",
    "    # Check if the directory exists, create it if it doesn't\n",
    "    if not os.path.exists(combined_path):\n",
    "        os.makedirs(combined_path)\n",
    "    \n",
    "    df.to_csv(os.path.join(combined_path, file_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772f7365",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in all the folds and their train/val/test preprocessed splits\n",
    "fold_path = os.path.join(current_working_directory, \"data\")\n",
    "\n",
    "f1_train_df=pd.read_csv(f\"{fold_path}/Fold1/train_preprocessed.csv\")\n",
    "f1_val_df=pd.read_csv(f\"{fold_path}/Fold1/vali_preprocessed.csv\")\n",
    "f1_test_df=pd.read_csv(f\"{fold_path}/Fold1/test_preprocessed.csv\")\n",
    "\n",
    "f2_train_df=pd.read_csv(f\"{fold_path}/Fold2/train_preprocessed.csv\")\n",
    "f2_val_df=pd.read_csv(f\"{fold_path}/Fold2/vali_preprocessed.csv\")\n",
    "f2_test_df=pd.read_csv(f\"{fold_path}/Fold2/test_preprocessed.csv\")\n",
    "\n",
    "f3_train_df=pd.read_csv(f\"{fold_path}/Fold3/train_preprocessed.csv\")\n",
    "f3_val_df=pd.read_csv(f\"{fold_path}/Fold3/vali_preprocessed.csv\")\n",
    "f3_test_df=pd.read_csv(f\"{fold_path}/Fold3/test_preprocessed.csv\")\n",
    "\n",
    "f4_train_df=pd.read_csv(f\"{fold_path}/Fold4/train_preprocessed.csv\")\n",
    "f4_val_df=pd.read_csv(f\"{fold_path}/Fold4/vali_preprocessed.csv\")\n",
    "f4_test_df=pd.read_csv(f\"{fold_path}/Fold4/test_preprocessed.csv\")\n",
    "\n",
    "f5_train_df=pd.read_csv(f\"{fold_path}/Fold5/train_preprocessed.csv\")\n",
    "f5_val_df=pd.read_csv(f\"{fold_path}/Fold5/vali_preprocessed.csv\")\n",
    "f5_test_df=pd.read_csv(f\"{fold_path}/Fold5/test_preprocessed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f9fc2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine each split into a train/val/test dataframe\n",
    "train_df=pd.concat([f1_train_df, f2_train_df, f3_train_df], ignore_index=True, axis=0).reset_index(drop=True)\n",
    "val_df=pd.concat([f1_val_df, f2_val_df, f3_val_df], ignore_index=True, axis=0).reset_index(drop=True)\n",
    "test_df=pd.concat([f1_test_df, f2_test_df, f3_test_df], ignore_index=True, axis=0).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b5b677",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine validation and test datasets\n",
    "train_df=pd.concat([train_df, val_df], ignore_index=True, axis=0).reset_index(drop=True)\n",
    "\n",
    "# Output these to a directory 'Combined'\n",
    "output_to_path(train_df, \"train.csv\")\n",
    "output_to_path(test_df, \"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "becca991",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df=pd.read_csv(f\"{fold_path}/Combined/train.csv\", index_col=0)\n",
    "test_df=pd.read_csv(f\"{fold_path}/Combined/test.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8620489f",
   "metadata": {},
   "source": [
    "# Build Tensorflow Records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a62e827",
   "metadata": {},
   "outputs": [],
   "source": [
    "def float_feature(value):\n",
    "    \"\"\"Tensorflow feature values must be in this list form\"\"\"\n",
    "    return tf.train.Feature(float_list=tf.train.FloatList(value=[value]))\n",
    "\n",
    "def int64_feature(value):\n",
    "    \"\"\"Tensorflow feature values must be in this list form\"\"\"\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
    "\n",
    "def build_tfrs(df, features_df, file_name):\n",
    "    \n",
    "    relevance_array=np.expand_dims(df['relevance'], axis=1)\n",
    "    qid_array=np.expand_dims(df['qid'], axis=1)\n",
    "    features_array = df.iloc[:, 2:].to_numpy()\n",
    "\n",
    "    df_tfr = np.concatenate((relevance_array, \\\n",
    "                                          qid_array, \\\n",
    "                                          features_array), axis=1)\n",
    "    \n",
    "    # Extract column names\n",
    "    labels = np.array(features_df['cols'])\n",
    "\n",
    "    options = tf.io.TFRecordOptions(compression_type='GZIP')\n",
    "    \n",
    "    output_file_path=\"/Users/malik/Desktop/Kaggle/learn_to_rank/data/Combined\"\n",
    "    \n",
    "    output_file_path=os.path.join(output_file_path, file_name)\n",
    "    \n",
    "    with tf.io.TFRecordWriter(output_file_path) as writer:\n",
    "        \n",
    "        # Create Example list \n",
    "        elwc = input_pb2.ExampleListWithContext()\n",
    "        \n",
    "        # Save the last query id for filtering\n",
    "        last_query_id = None\n",
    "        \n",
    "        # Df length\n",
    "        df_len=len(df_tfr)\n",
    "\n",
    "        for row in range(df_tfr.shape[0]):\n",
    "\n",
    "            # Select data from each row\n",
    "            relevance_label, query_id, features = df_tfr[row,0],df_tfr[row,1],df_tfr[row,2:]\n",
    "\n",
    "            # Create Example Dict - mapping each feature to its value\n",
    "            example_dict = {\n",
    "               f'{feat_name}':float_feature(feat_val)\n",
    "               for feat_name, feat_val in zip(labels, features)\n",
    "            }\n",
    "\n",
    "            # Add in the relevance label as a int64\n",
    "            example_dict['relevance_label'] = int64_feature(int(relevance_label))\n",
    "\n",
    "            # Create Features\n",
    "            example = tf.train.Example(features=tf.train.Features(\n",
    "                                                     feature=example_dict)\n",
    "                                             )\n",
    "\n",
    "            # If its a new qid in the iteration\n",
    "            if query_id != last_query_id:\n",
    "\n",
    "                # If its not the first qid iteration - write the object to the file\n",
    "                if last_query_id != None:\n",
    "                    writer.write(elwc.SerializeToString())\n",
    "\n",
    "                # Reset the new qid as the last qid\n",
    "                last_query_id = query_id\n",
    "\n",
    "                # Create the example object\n",
    "                elwc = input_pb2.ExampleListWithContext()\n",
    "\n",
    "                # Append to the example object the example we built\n",
    "                elwc.examples.append(example)\n",
    "\n",
    "\n",
    "            # If its the same qid, append to that existing example object, the example\n",
    "            else:\n",
    "                elwc.examples.append(example)\n",
    "        \n",
    "        # Writing the final query\n",
    "        writer.write(elwc.SerializeToString())\n",
    "        \n",
    "        print (\"Done outputing to tfrecord\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6716396",
   "metadata": {},
   "outputs": [],
   "source": [
    "build_tfrs(train_df, features_df, \"train.tfrecords\")\n",
    "build_tfrs(test_df, features_df, \"test.tfrecords\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba108a7",
   "metadata": {},
   "source": [
    "# Build Out Pre-training Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2688ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============ INPUT CREATOR ====================\n",
    "\n",
    "context_feature_spec = {}\n",
    "\n",
    "example_spec = {feat: tf.io.FixedLenFeature(shape=(1,), \\\n",
    "                        dtype=tf.float32, default_value=0.0) \\\n",
    "                        for feat in list(features_df['cols'])}\n",
    "\n",
    "label_spec = ('relevance_label', \\\n",
    "                tf.io.FixedLenFeature(shape=(1,), \\\n",
    "                dtype=tf.int64, \\\n",
    "                default_value=-1))\n",
    "\n",
    "input_creator = tfr.keras.model.FeatureSpecInputCreator(\n",
    "    context_feature_spec, example_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "376ec49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============ PREPROCESSOR ====================\n",
    "\n",
    "# For each feature, apply a log1p transformation\n",
    "preprocessor_specs = {\n",
    "    **{name: lambda t: tf.math.log1p(t * tf.sign(t)) * tf.sign(t)\n",
    "       for name in example_spec.keys()}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c2d444",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============ SCORER ====================\n",
    "scorer = tfr.keras.model.DNNScorer(\n",
    "    hidden_layer_dims=[64, 32, 16],\n",
    "    output_units=1,\n",
    "    activation=tf.nn.relu,\n",
    "    use_batch_norm=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387b872b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============ MODEL STRUCTURE ====================\n",
    "model_builder = tfr.keras.model.ModelBuilder(\n",
    "    input_creator=input_creator,\n",
    "    preprocessor=tfr.keras.model.PreprocessorWithSpec(preprocessor_specs),\n",
    "    scorer=scorer,\n",
    "    mask_feature_name=\"list_mask\",\n",
    "    name=\"model_builder\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820da7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======= DATASET  HYPERPARAMETERS ==========\n",
    "combined_train_path = os.path.join(current_working_directory, \"data\", \"combined\",\"train.tfrecords\")\n",
    "combined_test_path = os.path.join(current_working_directory, \"data\", \"combined\",\"test.tfrecords\")\n",
    "\n",
    "dataset_hparams = tfr.keras.pipeline.DatasetHparams(\n",
    "    train_input_pattern=combined_train_path,\n",
    "    valid_input_pattern=combined_test_path,\n",
    "    train_batch_size=32,\n",
    "    valid_batch_size=32,\n",
    "    list_size=50,\n",
    "    dataset_reader=tf.data.TFRecordDataset)\n",
    "\n",
    "# ======= DATASET BUILDER ==========\n",
    "dataset_builder = tfr.keras.pipeline.SimpleDatasetBuilder(\n",
    "    context_feature_spec,\n",
    "    example_spec,\n",
    "    mask_feature_name=\"list_mask\",\n",
    "    label_spec=label_spec,\n",
    "    hparams=dataset_hparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb450ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======= MODEL HYPERPARAMETERS ==========\n",
    "combined_path = os.path.join(current_working_directory, \"data\", \"combined\")\n",
    "\n",
    "pipeline_hparams = tfr.keras.pipeline.PipelineHparams(\n",
    "    model_dir=combined_path,\n",
    "    num_epochs=5,\n",
    "    steps_per_epoch=1000,\n",
    "    validation_steps=100,\n",
    "    learning_rate=0.05,\n",
    "    loss=\"approx_ndcg_loss\",\n",
    "    strategy=\"MirroredStrategy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff478b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======= RANKING PIPELINE ==========\n",
    "ranking_pipeline = tfr.keras.pipeline.SimplePipeline(\n",
    "    model_builder,\n",
    "    dataset_builder=dataset_builder,\n",
    "    hparams=pipeline_hparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e323de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======= TRAIN RANKING PIPELINE ==========\n",
    "ranking_pipeline.train_and_validate(verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0b1319",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======= RUN ON TEST SET ==========\n",
    "\n",
    "def compute_ndcg(dataset, model):\n",
    "    ndcg_metric = tfr.keras.metrics.NDCGMetric(name=\"ndcg_metric\")\n",
    "    for x, y in dataset:\n",
    "        scores = model.predict(x)\n",
    "        min_score = tf.reduce_min(scores)\n",
    "        scores = tf.where(tf.greater_equal(y, 0.), scores, min_score - 1e-5)\n",
    "        ndcg_metric.update_state(y_true=y, y_pred=scores)\n",
    "    return ndcg_metric.result().numpy()\n",
    "\n",
    "ds_test = dataset_builder.build_valid_dataset()\n",
    "\n",
    "# Get input features from the first batch of the test data\n",
    "for x, y in ds_test.take(1):\n",
    "    break\n",
    "    \n",
    "loaded_model = tf.keras.models.load_model(f\"{combined_path}/export/latest_model\")\n",
    "\n",
    "# Compute NDCG for the test set\n",
    "ndcg_score = compute_ndcg(ds_test, loaded_model)\n",
    "print(\"NDCG Score on Test Set: \", ndcg_score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_env",
   "language": "python",
   "name": "tf_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
